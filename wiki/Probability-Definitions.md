Probability Definitions
===

Shannon Entropy
---

$$
H(p) = - \sum_{x} p(x) \lg p(x)
$$

Cross Entropy
---

$$
H(p,q) = - \sum_{x} p(x) \lg q(x)
$$

Kullback-Leilbler Divergence
---

$$
\begin{array}{ll}
D_{KL} (p ||q) & = - \sum_{k} p(x) \lg \frac{q(x)}{p(x)} \\\\
 & = - \left( \sum_{x} p(x) \lg q(x) - p(x) \lg p(x) \right)
\end{array}
$$

Maximum Likelihood Estimation
---

$$
\text
$$

###### 2020-06-12
